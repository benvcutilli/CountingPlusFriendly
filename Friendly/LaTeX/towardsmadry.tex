\cite{madry2019deep}'s goal was to find both a defense and an attack mechanism that acted as some
kind of an upper bound for their respective roles, at least from the standpoint of noise being
within an $L_{\infty}$ norm set of boundaries. They proposed adversarial training (which is when the
network is trained on data that has been compromised) but a more advanced form of it. Adversarial
training has been tried previously (most prominently, and probably originally, in
\cite{goodfellow2015explaining}). However, as demonstrated in \cite{madry2019deep},
\cite{goodfellow2015explaining}'s procedure is not a sufficient procedure to ensure that one's model
can handle adversarial examples.

They start out their search by analytically encoding the adversarial training issue in the form of a
loss function that, when this function's value is reduced, is equivalent to min-max optimization.  In
this scenario, the ``max'' is the adversary maximizing the loss function, while the ``min'' is the
defender minimizing that maximum. The question they then pose is: what kind of adversary can
actually find something at least close to the maximum loss when generating an advesarial example? It
turns out that
\begin{enumerate}
    \item doing repeated FGSM~\cite{goodfellow2015explaining} steps a certain number of times on the
       respective output of the last step, clipping to be within the allowed ball of noise, and then
    \item trying that loop several times but starting at uniformly random locations within the $L_{inf}$
       ball of allowed noise around the image
\end{enumerate}
reveals that the set of images generated by this method has little variance in loss value. They
refer to this method as \textit{Projected Gradient Descent} (PGD). The conclusion that they draw is
that
\begin{quote}
    \textit{...our exploration with PGD does not preclude the existence of some
    isolated maxima with much larger function value. However, our experiments suggest that such
    better local maxima are hard to find with first order methods...}
    --~\cite{madry2019deep}[section 3.2]
\end{quote}
and that, as a result, PGD (with the aforementioned randomization and, to the best of this author's
understanding, choosing the best example out of all of them) likely gives us something close to the
best attack possible.

Assuming that their assumptions are correct, this conclusion (alongside other point(s) made in the
paper) completes the requirements needed to make adversarial training an effective way to thwart
attacks. In order to empirically prove that adversarial training with PGD actually works, they
tested three different things: a white-box attack and two black-box attacks, each of which were
% Including the chapter listed in ^^^cifarsite^^ may have been what [7ffa04, bottom] desires, so
% it is included to be safe
attempted using multiple attack methods, both on MNIST\cite{lecun} and CIFAR-10\cite{cifar}[ch. 3].
Both as a defense and as an attack, PGD dominated the other methods. For adversarial training, only
one random point was selected, with their reasoning appearing to be that successive epochs will not
change the model enough such that only attacking once per epoch over several epochs will result in
roughly the same adversarial examples being generated using the full PGD attack per example during
one epoch. In comparison to FGSM-based adversarial training (modified from
\cite{goodfellow2015explaining} by having the training set consist only of perturbed images), there
were many instances in which, from the white-box perspective, any sort of robustness completely or
almost completely disappeared when using the CIFAR-10 dataset. Black-box attacks caused a less
dramatic decrease in model performance, but it was still significant. White-box attacks on models
with PGD-based adversarial training were only mildly successful on MNIST using basically any attack;
however, for CIFAR, ``[t]he adversarial robustness of our network is significant, given the power of
iterative adversaries, but still far from satisfactory''~\cite{madry2019deep}[page 12], nowhere near
matching the MNIST network when attacked. Typically, attacking with PGD resulted in the lowest
scores, and PGD with multiple candidate adversarial examples per image proved to be the most
detrimental. There was only one scenario in which FGSM was more effective as an attacker, and that
was in a black-box setting with a model from a different paper, \cite{tramèr2017space}. To wrap up,
they conclude that attacks like FGSM are not adequate by pointing out~\cite{madry2019deep}[§ 6] that
the fact that adversarially-trained networks that use FGSM cannot resist PGD, and, in the same
section, point to \cite{tramèr2017space}\footnote{The author has not read this paper} as proof that
non-linear attacks may be more viable the further out the adversarial example is from the sample.
